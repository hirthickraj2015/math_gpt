{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing Notebook\n",
    "\n",
    "## CS7CS4 Machine Learning - Final Assignment 2025-26\n",
    "\n",
    "This notebook allows you to test your trained Math and Boolean GPT models with custom prompts.\n",
    "\n",
    "### Features:\n",
    "- Load trained models\n",
    "- Test with custom prompts\n",
    "- Batch testing\n",
    "- Generate examples for report\n",
    "- Interactive testing\n",
    "\n",
    "### Usage:\n",
    "1. Run all cells in order\n",
    "2. Choose which model to test (Math or Boolean)\n",
    "3. Enter your prompts\n",
    "4. View and analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Definition\n",
    "\n",
    "We need to define the same architecture used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT Language Model.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"Generate new tokens.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_model_and_vocab(checkpoint_path, model_weights_path=None):\n",
    "    \"\"\"\n",
    "    Load model and vocabulary from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: path to complete checkpoint (with vocab)\n",
    "        model_weights_path: alternative path to just weights file\n",
    "    \n",
    "    Returns:\n",
    "        model: loaded model\n",
    "        encode: encoding function\n",
    "        decode: decoding function\n",
    "        vocab_info: vocabulary information\n",
    "    \"\"\"\n",
    "    # Try to load complete checkpoint first\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Extract hyperparameters\n",
    "        hyperparams = checkpoint.get('hyperparameters', {})\n",
    "        vocab_size = checkpoint['vocab_size']\n",
    "        chars = checkpoint['chars']\n",
    "        stoi = checkpoint['stoi']\n",
    "        itos = checkpoint['itos']\n",
    "        \n",
    "        # Create model\n",
    "        model = GPTLanguageModel(\n",
    "            vocab_size=vocab_size,\n",
    "            n_embd=hyperparams.get('n_embd', 128),\n",
    "            n_head=hyperparams.get('n_head', 4),\n",
    "            n_layer=hyperparams.get('n_layer', 4),\n",
    "            block_size=hyperparams.get('block_size', 32),\n",
    "            dropout=hyperparams.get('dropout', 0.1)\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully\")\n",
    "        print(f\"  Vocabulary size: {vocab_size}\")\n",
    "        print(f\"  Embedding dim: {hyperparams.get('n_embd', 128)}\")\n",
    "        print(f\"  Layers: {hyperparams.get('n_layer', 4)}\")\n",
    "        print(f\"  Heads: {hyperparams.get('n_head', 4)}\")\n",
    "        \n",
    "    elif model_weights_path and os.path.exists(model_weights_path):\n",
    "        print(f\"Loading weights from: {model_weights_path}\")\n",
    "        print(\"Warning: Loading weights only, vocabulary must be loaded separately\")\n",
    "        raise ValueError(\"Please use complete checkpoint with vocabulary\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    \n",
    "    # Create encode/decode functions\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "    \n",
    "    vocab_info = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'chars': chars,\n",
    "        'stoi': stoi,\n",
    "        'itos': itos\n",
    "    }\n",
    "    \n",
    "    return model, encode, decode, vocab_info\n",
    "\n",
    "print(\"Loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing functions defined\n"
     ]
    }
   ],
   "source": [
    "def test_single_prompt(model, prompt, encode, decode, max_tokens=30, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Test model with a single prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        prompt: input prompt (e.g., \"5+3=\")\n",
    "        encode: encoding function\n",
    "        decode: decoding function\n",
    "        max_tokens: maximum tokens to generate\n",
    "        temperature: sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        full_output: complete generated text\n",
    "        answer: extracted answer only\n",
    "    \"\"\"\n",
    "    # Ensure prompt ends with '='\n",
    "    if '=' not in prompt:\n",
    "        prompt = prompt + '='\n",
    "    \n",
    "    # Encode and generate\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(context, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    \n",
    "    # Decode output\n",
    "    full_output = decode(generated[0].tolist())\n",
    "    \n",
    "    # Extract answer (after '=' and before newline)\n",
    "    if '=' in full_output:\n",
    "        answer = full_output.split('=')[-1].split('\\n')[0].strip()\n",
    "    else:\n",
    "        answer = full_output.strip()\n",
    "    \n",
    "    return full_output, answer\n",
    "\n",
    "\n",
    "def test_multiple_prompts(model, prompts, encode, decode, max_tokens=30, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Test model with multiple prompts.\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        prompts: list of input prompts\n",
    "        encode: encoding function\n",
    "        decode: decoding function\n",
    "        max_tokens: maximum tokens to generate\n",
    "        temperature: sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        results: list of (prompt, full_output, answer) tuples\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        full_output, answer = test_single_prompt(\n",
    "            model, prompt, encode, decode, max_tokens, temperature\n",
    "        )\n",
    "        results.append((prompt, full_output, answer))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results, expected_answers=None):\n",
    "    \"\"\"\n",
    "    Print test results in a formatted table.\n",
    "    \n",
    "    Args:\n",
    "        results: list of (prompt, full_output, answer) tuples\n",
    "        expected_answers: optional list of expected answers for comparison\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if expected_answers:\n",
    "        print(f\"\\n{'Prompt':<25} {'Expected':<15} {'Predicted':<15} {'Status'}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for i, (prompt, full_output, answer) in enumerate(results):\n",
    "            if '=' in prompt:\n",
    "                display_prompt = prompt.split('=')[0] + '='\n",
    "            else:\n",
    "                display_prompt = prompt\n",
    "            \n",
    "            expected = expected_answers[i] if i < len(expected_answers) else \"?\"\n",
    "            status = \"✓\" if answer == expected else \"✗\"\n",
    "            \n",
    "            print(f\"{display_prompt:<25} {expected:<15} {answer:<15} {status}\")\n",
    "    else:\n",
    "        print(f\"\\n{'Prompt':<30} {'Predicted Answer'}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for prompt, full_output, answer in results:\n",
    "            if '=' in prompt:\n",
    "                display_prompt = prompt.split('=')[0] + '='\n",
    "            else:\n",
    "                display_prompt = prompt\n",
    "            \n",
    "            print(f\"{display_prompt:<30} {answer}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate accuracy if expected answers provided\n",
    "    if expected_answers:\n",
    "        correct = sum(1 for i, (_, _, answer) in enumerate(results) \n",
    "                     if i < len(expected_answers) and answer == expected_answers[i])\n",
    "        total = min(len(results), len(expected_answers))\n",
    "        accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "        print(f\"\\nAccuracy: {correct}/{total} ({accuracy:.1f}%)\")\n",
    "\n",
    "\n",
    "def save_results_to_file(results, filename, expected_answers=None):\n",
    "    \"\"\"\n",
    "    Save test results to a text file for report appendix.\n",
    "    \n",
    "    Args:\n",
    "        results: list of (prompt, full_output, answer) tuples\n",
    "        filename: output filename\n",
    "        expected_answers: optional expected answers\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"MODEL TEST RESULTS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (prompt, full_output, answer) in enumerate(results):\n",
    "            if '=' in prompt:\n",
    "                display_prompt = prompt.split('=')[0] + '='\n",
    "            else:\n",
    "                display_prompt = prompt\n",
    "            \n",
    "            f.write(f\"Test {i+1}:\\n\")\n",
    "            f.write(f\"  Input:     {display_prompt}\\n\")\n",
    "            f.write(f\"  Predicted: {answer}\\n\")\n",
    "            \n",
    "            if expected_answers and i < len(expected_answers):\n",
    "                expected = expected_answers[i]\n",
    "                status = \"✓\" if answer == expected else \"✗\"\n",
    "                f.write(f\"  Expected:  {expected} {status}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        if expected_answers:\n",
    "            correct = sum(1 for i, (_, _, answer) in enumerate(results) \n",
    "                         if i < len(expected_answers) and answer == expected_answers[i])\n",
    "            total = min(len(results), len(expected_answers))\n",
    "            accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "            f.write(f\"\\nAccuracy: {correct}/{total} ({accuracy:.1f}%)\\n\")\n",
    "    \n",
    "    print(f\"Results saved to: {filename}\")\n",
    "\n",
    "print(\"Testing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model\n",
    "\n",
    "Choose which model to test by uncommenting the appropriate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Math GPT...\n",
      "\n",
      "Loading checkpoint from: checkpoints/best_model.pt\n",
      "Model loaded successfully\n",
      "  Vocabulary size: 19\n",
      "  Embedding dim: 128\n",
      "  Layers: 4\n",
      "  Heads: 4\n",
      "\n",
      "Math GPT ready for testing!\n",
      "Vocabulary: \n",
      " % ( ) * + - / 0 1 2 3 4 5 6 7 8 9 =\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE MODEL TO TEST\n",
    "# Uncomment ONE of the following:\n",
    "\n",
    "# Option 1: Math GPT\n",
    "checkpoint_path = 'checkpoints/best_model.pt'  # or 'part1_complete_checkpoint.pt'\n",
    "model_type = 'Math GPT'\n",
    "\n",
    "# Option 2: Boolean GPT (uncomment when you have this model)\n",
    "# checkpoint_path = 'checkpoints_boolean/best_model.pt'\n",
    "# model_type = 'Boolean GPT'\n",
    "\n",
    "print(f\"Loading {model_type}...\\n\")\n",
    "\n",
    "try:\n",
    "    model, encode, decode, vocab_info = load_model_and_vocab(checkpoint_path)\n",
    "    print(f\"\\n{model_type} ready for testing!\")\n",
    "    print(f\"Vocabulary: {' '.join(vocab_info['chars'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"\\nMake sure you have:\")\n",
    "    print(\"  1. Trained the model (run part1_math_gpt.ipynb)\")\n",
    "    print(\"  2. The checkpoint file exists at the specified path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test with Math Prompts\n",
    "\n",
    "Test the Math GPT model with various arithmetic expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Math GPT with 21 prompts...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST RESULTS\n",
      "================================================================================\n",
      "\n",
      "Prompt                    Expected        Predicted       Status\n",
      "--------------------------------------------------------------------------------\n",
      "5+3                       8               0               ✗\n",
      "12+7                      19              2               ✗\n",
      "25+38                     63              83              ✗\n",
      "10-3                      7               38              ✗\n",
      "20-15                     5               0               ✗\n",
      "50-27                     23              104             ✗\n",
      "6*8                       48                              ✗\n",
      "7*7                       49                              ✗\n",
      "12*3                      36              -1              ✗\n",
      "20//4                     5               88              ✗\n",
      "15//3                     5               24              ✗\n",
      "100//10                   10              -39             ✗\n",
      "17%5                      2               31              ✗\n",
      "20%6                      2               40              ✗\n",
      "15%4                      3               58              ✗\n",
      "(3+2)*4                   20              20              ✓\n",
      "(10-3)*2                  14              2               ✗\n",
      "(5+5)//2                  5               5082            ✗\n",
      "10+(5*2)                  20              112             ✗\n",
      "20-(3*4)                  8               22              ✗\n",
      "((2+3)*4)+1               21              -9              ✗\n",
      "================================================================================\n",
      "\n",
      "Accuracy: 1/21 (4.8%)\n",
      "Results saved to: math_test_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Define test prompts for Math GPT\n",
    "math_prompts = [\n",
    "    # Simple addition\n",
    "    \"5+3\",\n",
    "    \"12+7\",\n",
    "    \"25+38\",\n",
    "    \n",
    "    # Simple subtraction\n",
    "    \"10-3\",\n",
    "    \"20-15\",\n",
    "    \"50-27\",\n",
    "    \n",
    "    # Simple multiplication\n",
    "    \"6*8\",\n",
    "    \"7*7\",\n",
    "    \"12*3\",\n",
    "    \n",
    "    # Simple division\n",
    "    \"20//4\",\n",
    "    \"15//3\",\n",
    "    \"100//10\",\n",
    "    \n",
    "    # Modulo\n",
    "    \"17%5\",\n",
    "    \"20%6\",\n",
    "    \"15%4\",\n",
    "    \n",
    "    # With parentheses\n",
    "    \"(3+2)*4\",\n",
    "    \"(10-3)*2\",\n",
    "    \"(5+5)//2\",\n",
    "    \n",
    "    # More complex\n",
    "    \"10+(5*2)\",\n",
    "    \"20-(3*4)\",\n",
    "    \"((2+3)*4)+1\",\n",
    "]\n",
    "\n",
    "# Expected answers (for verification)\n",
    "math_expected = [\n",
    "    \"8\", \"19\", \"63\",           # addition\n",
    "    \"7\", \"5\", \"23\",             # subtraction\n",
    "    \"48\", \"49\", \"36\",           # multiplication\n",
    "    \"5\", \"5\", \"10\",             # division\n",
    "    \"2\", \"2\", \"3\",              # modulo\n",
    "    \"20\", \"14\", \"5\",            # parentheses\n",
    "    \"20\", \"8\", \"21\",            # complex\n",
    "]\n",
    "\n",
    "print(f\"Testing {model_type} with {len(math_prompts)} prompts...\\n\")\n",
    "\n",
    "# Run tests\n",
    "math_results = test_multiple_prompts(\n",
    "    model, math_prompts, encode, decode,\n",
    "    max_tokens=20, temperature=0.7\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print_results(math_results, math_expected)\n",
    "\n",
    "# Save to file\n",
    "save_results_to_file(math_results, 'math_test_results.txt', math_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Boolean Prompts\n",
    "\n",
    "Test the Boolean GPT model with various logic expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts for Boolean GPT\n",
    "boolean_prompts = [\n",
    "    # Simple AND\n",
    "    \"True AND True\",\n",
    "    \"True AND False\",\n",
    "    \"False AND False\",\n",
    "    \n",
    "    # Simple OR\n",
    "    \"True OR True\",\n",
    "    \"True OR False\",\n",
    "    \"False OR False\",\n",
    "    \n",
    "    # Simple XOR\n",
    "    \"True XOR True\",\n",
    "    \"True XOR False\",\n",
    "    \"False XOR False\",\n",
    "    \n",
    "    # Simple NOT\n",
    "    \"NOT True\",\n",
    "    \"NOT False\",\n",
    "    \n",
    "    # With parentheses\n",
    "    \"(True OR False) AND True\",\n",
    "    \"(True AND False) OR True\",\n",
    "    \"NOT (True AND False)\",\n",
    "    \n",
    "    # Complex\n",
    "    \"(True XOR False) AND True\",\n",
    "    \"NOT True OR False\",\n",
    "    \"(NOT True) AND (NOT False)\",\n",
    "]\n",
    "\n",
    "# Expected answers\n",
    "boolean_expected = [\n",
    "    \"True\", \"False\", \"False\",    # AND\n",
    "    \"True\", \"True\", \"False\",     # OR\n",
    "    \"False\", \"True\", \"False\",    # XOR\n",
    "    \"False\", \"True\",             # NOT\n",
    "    \"True\", \"True\", \"True\",      # parentheses\n",
    "    \"True\", \"False\", \"False\",    # complex\n",
    "]\n",
    "\n",
    "# Only run if testing Boolean model\n",
    "if model_type == 'Boolean GPT':\n",
    "    print(f\"Testing {model_type} with {len(boolean_prompts)} prompts...\\n\")\n",
    "    \n",
    "    boolean_results = test_multiple_prompts(\n",
    "        model, boolean_prompts, encode, decode,\n",
    "        max_tokens=20, temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print_results(boolean_results, boolean_expected)\n",
    "    save_results_to_file(boolean_results, 'boolean_test_results.txt', boolean_expected)\n",
    "else:\n",
    "    print(\"Skipping boolean tests (Math model loaded)\")\n",
    "    print(\"To test Boolean model, change checkpoint_path in Section 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing\n",
    "\n",
    "Enter your own custom prompts to test the model interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_test():\n",
    "    \"\"\"Interactive testing loop.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERACTIVE TESTING MODE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTesting {model_type}\")\n",
    "    print(\"\\nEnter prompts to test (or 'quit' to exit)\")\n",
    "    \n",
    "    if model_type == 'Math GPT':\n",
    "        print(\"Examples: 5+3, 12-7, (3+2)*4\")\n",
    "    else:\n",
    "        print(\"Examples: True AND False, NOT True, (True OR False) AND True\")\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    custom_results = []\n",
    "    \n",
    "    while True:\n",
    "        prompt = input(\"\\nEnter prompt (or 'quit'): \").strip()\n",
    "        \n",
    "        if prompt.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        \n",
    "        if not prompt:\n",
    "            continue\n",
    "        \n",
    "        # Test the prompt\n",
    "        full_output, answer = test_single_prompt(\n",
    "            model, prompt, encode, decode,\n",
    "            max_tokens=30, temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Display result\n",
    "        if '=' in prompt:\n",
    "            display_prompt = prompt.split('=')[0] + '='\n",
    "        else:\n",
    "            display_prompt = prompt + '='\n",
    "        \n",
    "        print(f\"  Input:    {display_prompt}\")\n",
    "        print(f\"  Output:   {answer}\")\n",
    "        \n",
    "        custom_results.append((prompt, full_output, answer))\n",
    "    \n",
    "    # Save custom results if any\n",
    "    if custom_results:\n",
    "        print(f\"\\nTested {len(custom_results)} custom prompts\")\n",
    "        save_choice = input(\"Save results to file? (y/n): \").strip().lower()\n",
    "        \n",
    "        if save_choice == 'y':\n",
    "            filename = f\"custom_{model_type.lower().replace(' ', '_')}_results.txt\"\n",
    "            save_results_to_file(custom_results, filename)\n",
    "    \n",
    "    print(\"\\nExiting interactive mode\")\n",
    "\n",
    "# Run interactive testing\n",
    "# Uncomment the next line to start interactive mode\n",
    "# interactive_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Testing from File\n",
    "\n",
    "Test the model with prompts loaded from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_from_file(filepath, has_answers=True):\n",
    "    \"\"\"\n",
    "    Test model with prompts from a file.\n",
    "    \n",
    "    File format:\n",
    "        With answers: prompt=answer (e.g., \"5+3=8\")\n",
    "        Without answers: just prompts (e.g., \"5+3\")\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to file with prompts\n",
    "        has_answers: whether file contains expected answers\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    if has_answers:\n",
    "        # Split into prompts and expected answers\n",
    "        prompts = []\n",
    "        expected = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if '=' in line:\n",
    "                parts = line.split('=')\n",
    "                prompts.append(parts[0] + '=')\n",
    "                expected.append(parts[1] if len(parts) > 1 else '')\n",
    "            else:\n",
    "                prompts.append(line)\n",
    "                expected.append('')\n",
    "    else:\n",
    "        prompts = lines\n",
    "        expected = None\n",
    "    \n",
    "    print(f\"Loaded {len(prompts)} prompts from {filepath}\\n\")\n",
    "    \n",
    "    # Test prompts\n",
    "    results = test_multiple_prompts(\n",
    "        model, prompts, encode, decode,\n",
    "        max_tokens=30, temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print_results(results, expected)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = filepath.replace('.txt', '_results.txt')\n",
    "    save_results_to_file(results, output_file, expected)\n",
    "\n",
    "# Example usage:\n",
    "# test_from_file('my_test_prompts.txt', has_answers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Examples for Report\n",
    "\n",
    "Generate a comprehensive set of examples showing both strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report_examples(num_correct=10, num_incorrect=10):\n",
    "    \"\"\"\n",
    "    Generate examples for report appendix.\n",
    "    Tries to find both correct and incorrect predictions.\n",
    "    \"\"\"\n",
    "    print(\"Generating examples for report...\\n\")\n",
    "    \n",
    "    if model_type == 'Math GPT':\n",
    "        # Load test dataset\n",
    "        test_path = 'dataset/math/testing/math_test.txt'\n",
    "    else:\n",
    "        test_path = 'dataset/boolean/testing/boolean_test.txt'\n",
    "    \n",
    "    if not os.path.exists(test_path):\n",
    "        print(f\"Test dataset not found: {test_path}\")\n",
    "        return\n",
    "    \n",
    "    with open(test_path, 'r') as f:\n",
    "        test_expressions = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    # Sample random expressions\n",
    "    import random\n",
    "    sample_size = min(200, len(test_expressions))\n",
    "    samples = random.sample(test_expressions, sample_size)\n",
    "    \n",
    "    correct_examples = []\n",
    "    incorrect_examples = []\n",
    "    \n",
    "    print(f\"Testing {sample_size} samples to find good examples...\")\n",
    "    \n",
    "    for expr in samples:\n",
    "        if '=' not in expr:\n",
    "            continue\n",
    "        \n",
    "        parts = expr.split('=')\n",
    "        prompt = parts[0] + '='\n",
    "        expected = parts[1] if len(parts) > 1 else ''\n",
    "        \n",
    "        # Test\n",
    "        _, answer = test_single_prompt(\n",
    "            model, prompt, encode, decode,\n",
    "            max_tokens=20, temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Categorize\n",
    "        if answer == expected:\n",
    "            if len(correct_examples) < num_correct:\n",
    "                correct_examples.append((prompt, expected, answer, True))\n",
    "        else:\n",
    "            if len(incorrect_examples) < num_incorrect:\n",
    "                incorrect_examples.append((prompt, expected, answer, False))\n",
    "        \n",
    "        # Stop if we have enough\n",
    "        if len(correct_examples) >= num_correct and len(incorrect_examples) >= num_incorrect:\n",
    "            break\n",
    "    \n",
    "    # Combine results\n",
    "    all_examples = correct_examples + incorrect_examples\n",
    "    expected_answers = [exp for _, exp, _, _ in all_examples]\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nFound:\")\n",
    "    print(f\"  Correct examples: {len(correct_examples)}\")\n",
    "    print(f\"  Incorrect examples: {len(incorrect_examples)}\")\n",
    "    \n",
    "    # Save to file\n",
    "    filename = f\"{model_type.lower().replace(' ', '_')}_report_examples.txt\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"EXAMPLES FOR REPORT APPENDIX - {model_type}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"STRENGTHS (Correct Predictions):\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for prompt, expected, answer, _ in correct_examples:\n",
    "            f.write(f\"Input:    {prompt}\\n\")\n",
    "            f.write(f\"Expected: {expected}\\n\")\n",
    "            f.write(f\"Output:   {answer} ✓\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"\\nWEAKNESSES (Incorrect Predictions):\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for prompt, expected, answer, _ in incorrect_examples:\n",
    "            f.write(f\"Input:    {prompt}\\n\")\n",
    "            f.write(f\"Expected: {expected}\\n\")\n",
    "            f.write(f\"Output:   {answer} ✗\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"\\nExamples saved to: {filename}\")\n",
    "    print(\"Use these examples in your report appendix!\")\n",
    "\n",
    "# Generate examples\n",
    "generate_report_examples(num_correct=15, num_incorrect=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "This notebook provides comprehensive testing capabilities for your trained models:\n",
    "\n",
    "1. **Single prompt testing** - Test individual expressions\n",
    "2. **Batch testing** - Test multiple prompts at once\n",
    "3. **Interactive mode** - Enter custom prompts\n",
    "4. **File-based testing** - Load prompts from files\n",
    "5. **Report generation** - Generate examples for your report\n",
    "\n",
    "### Files Generated:\n",
    "- `math_test_results.txt` - Math model test results\n",
    "- `boolean_test_results.txt` - Boolean model test results\n",
    "- `*_report_examples.txt` - Examples for report appendix\n",
    "\n",
    "### Tips:\n",
    "- Use **temperature** parameter to control randomness (0.5-1.0)\n",
    "- Lower temperature = more deterministic outputs\n",
    "- Higher temperature = more varied outputs\n",
    "- Test with diverse examples to understand model behavior\n",
    "\n",
    "### For Your Report:\n",
    "Use the generated examples to demonstrate:\n",
    "- Model strengths (operations it handles well)\n",
    "- Model weaknesses (where it fails)\n",
    "- Error patterns\n",
    "- Comparative analysis between Math and Boolean models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

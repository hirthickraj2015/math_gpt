{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Boolean GPT - Boolean Logic Evaluator\n",
    "## CS7CS4 Machine Learning - Final Assignment 2025-26\n",
    "\n",
    "This notebook implements a transformer-based model for evaluating boolean expressions.\n",
    "\n",
    "### Tasks Covered:\n",
    "- **Task 2.1**: Build appropriate datasets (see 1_dataset_generation.ipynb)\n",
    "- **Task 2.2**: Define appropriate evaluation metrics\n",
    "- **Task 2.3**: Explore architectural adaptations  \n",
    "- **Task 2.4**: Analyze performance across different boolean operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "### Architectural Choices (Task 2.3):\n",
    "\n",
    "Boolean logic has an even **smaller state space** than arithmetic:\n",
    "\n",
    "1. **Block size = 48**: Boolean expressions can be longer (\"True AND False\")\n",
    "2. **Embedding dimension = 32**: Very small vocabulary (only True, False, operators)\n",
    "3. **2 layers, 2 heads**: Boolean logic is simpler than arithmetic\n",
    "4. **Dropout = 0.05**: Minimal regularization needed\n",
    "5. **Learning rate = 5e-4**: Slightly higher for fast convergence\n",
    "6. **Word-level tokenization**: \"True\", \"False\", \"AND\", etc. are atomic units\n",
    "\n",
    "**Rationale**: Boolean operations are deterministic with only 2 values. A very small model suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters - optimized for boolean logic\n",
    "batch_size = 128         # Large batches\n",
    "block_size = 48          # Longer for \"True AND False\" etc.\n",
    "max_iters = 3000         # Fewer iterations needed\n",
    "eval_interval = 300      # Frequent evaluation\n",
    "learning_rate = 5e-4     # Higher learning rate\n",
    "eval_iters = 100         # Loss estimation\n",
    "n_embd = 32              # Very small embeddings\n",
    "n_head = 2               # 2 heads\n",
    "n_layer = 2              # 2 layers\n",
    "dropout = 0.05           # Minimal dropout\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Block size: {block_size}\")\n",
    "print(f\"  Embedding dim: {n_embd}\")\n",
    "print(f\"  Layers: {n_layer}, Heads: {n_head}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "### Task 2.1: Dataset Details\n",
    "\n",
    "Our boolean dataset includes:\n",
    "- **Four operations**: AND, OR, XOR, NOT\n",
    "- **Two values**: True, False\n",
    "- **Exhaustive coverage** of all combinations\n",
    "- **Parentheses** for complex expressions\n",
    "- **High repetition** (smaller state space needs more examples)\n",
    "- **90/10 train/test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "with open('dataset/boolean/training/boolean_train.txt', 'r') as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "with open('dataset/boolean/testing/boolean_test.txt', 'r') as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "# Create vocabulary  \n",
    "chars = sorted(list(set(train_text + test_text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Prepare tensors\n",
    "train_data = torch.tensor(encode(train_text), dtype=torch.long)\n",
    "test_data = torch.tensor(encode(test_text), dtype=torch.long)\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Characters: {''.join(chars)}\")\n",
    "print(f\"  Training size: {len(train_text):,} chars ({train_text.count(chr(10)):,} expressions)\")\n",
    "print(f\"  Testing size: {len(test_text):,} chars ({test_text.count(chr(10)):,} expressions)\")\n",
    "print(f\"\\nSample expressions:\")\n",
    "for line in train_text.split('\\n')[:10]:\n",
    "    print(f\"  {line}\")\n",
    "\n",
    "# Data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### Task 2.3: Architectural Comparison\n",
    "\n",
    "**Differences from Math GPT**:\n",
    "- **Smaller embeddings** (32 vs 64): Boolean has fewer unique tokens\n",
    "- **Fewer heads** (2 vs 4): Simpler relationships to model\n",
    "- **Longer block size** (48 vs 32): Boolean expressions use more characters per operation\n",
    "- **Lower dropout** (0.05 vs 0.1): Less overfitting risk\n",
    "\n",
    "**Similarities**:\n",
    "- Same basic transformer architecture\n",
    "- Character-level tokenization\n",
    "- 2 layers (both are shallow networks)\n",
    "\n",
    "This demonstrates how architectural choices must adapt to task characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"Single self-attention head.\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple attention heads in parallel.\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network.\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\"GPT model for boolean logic.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=0.8):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# Initialize model\n",
    "model = GPTLanguageModel().to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nModel initialized:\")\n",
    "print(f\"  Parameters: {n_params:,} ({n_params/1e6:.3f}M)\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_losses, test_losses, iters = [], [], []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training...\\n\")\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_losses.append(losses['train'])\n",
    "        test_losses.append(losses['test'])\n",
    "        iters.append(iter)\n",
    "        print(f\"Iter {iter:4d} | Train: {losses['train']:.4f} | Test: {losses['test']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"\\nTraining complete in {time.time()-start_time:.1f}s\")\n",
    "print(f\"Final test loss: {test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task 2.2: Evaluation Metrics\n",
    "\n",
    "Similar to Math GPT, we use:\n",
    "\n",
    "1. **Exact Match Accuracy**: Complete correctness\n",
    "2. **Character-Level Accuracy**: Partial credit\n",
    "3. **Operation-Specific Accuracy**: AND, OR, XOR, NOT performance\n",
    "4. **Error Analysis**: Failure patterns\n",
    "\n",
    "These metrics reveal which logical operations are mastered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_expressions, max_samples=500):\n",
    "    \"\"\"Comprehensive boolean model evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    correct = 0\n",
    "    char_correct = 0\n",
    "    char_total = 0\n",
    "    \n",
    "    test_exprs = [e.strip() for e in test_expressions if '=' in e][:max_samples]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for expr in test_exprs:\n",
    "            parts = expr.split('=')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            \n",
    "            input_part = parts[0] + '='\n",
    "            expected = parts[1]\n",
    "            \n",
    "            try:\n",
    "                # Generate prediction\n",
    "                context = torch.tensor([encode(input_part)], dtype=torch.long, device=device)\n",
    "                generated = model.generate(context, max_new_tokens=10, temperature=0.8)\n",
    "                prediction = decode(generated[0].tolist())\n",
    "                \n",
    "                # Extract answer\n",
    "                if '=' in prediction:\n",
    "                    pred_answer = prediction.split('=', 1)[1].split('\\n')[0].strip()\n",
    "                else:\n",
    "                    pred_answer = \"\"\n",
    "                \n",
    "                is_correct = (pred_answer == expected)\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                \n",
    "                # Character-level accuracy\n",
    "                for i in range(max(len(expected), len(pred_answer))):\n",
    "                    char_total += 1\n",
    "                    if i < len(expected) and i < len(pred_answer) and expected[i] == pred_answer[i]:\n",
    "                        char_correct += 1\n",
    "                \n",
    "                results.append((input_part, expected, pred_answer, is_correct))\n",
    "            \n",
    "            except Exception as e:\n",
    "                results.append((input_part, expected, \"\", False))\n",
    "    \n",
    "    exact_match_acc = (correct / len(results)) * 100 if results else 0\n",
    "    char_acc = (char_correct / char_total) * 100 if char_total else 0\n",
    "    \n",
    "    return exact_match_acc, char_acc, results\n",
    "\n",
    "# Run evaluation\n",
    "test_expressions = test_text.split('\\n')\n",
    "exact_acc, char_acc, eval_results = evaluate_model(test_expressions, max_samples=500)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExact Match Accuracy: {exact_acc:.2f}%\")\n",
    "print(f\"Character-Level Accuracy: {char_acc:.2f}%\")\n",
    "print(f\"Total samples evaluated: {len(eval_results)}\")\n",
    "print(f\"Correct predictions: {sum(1 for r in eval_results if r[3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task 2.4: Operation-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_boolean_operation(expr):\n",
    "    \"\"\"Categorize boolean expression by operation.\"\"\"\n",
    "    expr_upper = expr.upper()\n",
    "    if '(' in expr:\n",
    "        return 'parentheses'\n",
    "    elif 'NOT' in expr_upper and ('AND' in expr_upper or 'OR' in expr_upper or 'XOR' in expr_upper):\n",
    "        return 'not_combined'\n",
    "    elif 'XOR' in expr_upper:\n",
    "        return 'xor'\n",
    "    elif 'AND' in expr_upper:\n",
    "        return 'and'\n",
    "    elif 'OR' in expr_upper:\n",
    "        return 'or'\n",
    "    elif 'NOT' in expr_upper:\n",
    "        return 'not'\n",
    "    return 'other'\n",
    "\n",
    "# Analyze by operation\n",
    "op_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "for inp, exp, pred, correct in eval_results:\n",
    "    op_type = categorize_boolean_operation(inp)\n",
    "    op_stats[op_type]['total'] += 1\n",
    "    if correct:\n",
    "        op_stats[op_type]['correct'] += 1\n",
    "\n",
    "print(\"\\nAccuracy by Operation Type:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Operation':<20} {'Correct':<10} {'Total':<10} {'Accuracy'}\")\n",
    "print(\"-\" * 70)\n",
    "for op in sorted(op_stats.keys()):\n",
    "    stats = op_stats[op]\n",
    "    acc = (stats['correct'] / stats['total']) * 100 if stats['total'] > 0 else 0\n",
    "    print(f\"{op:<20} {stats['correct']:<10} {stats['total']:<10} {acc:.1f}%\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nCorrect Predictions (Examples):\")\n",
    "correct_examples = [r for r in eval_results if r[3]][:10]\n",
    "for inp, exp, pred, _ in correct_examples:\n",
    "    print(f\"  {inp}{pred} ✓\")\n",
    "\n",
    "print(\"\\nIncorrect Predictions (Examples):\")\n",
    "incorrect_examples = [r for r in eval_results if not r[3]][:10]\n",
    "for inp, exp, pred, _ in incorrect_examples:\n",
    "    print(f\"  {inp}{pred} ✗ (expected: {exp})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(iters, train_losses, label='Train', linewidth=2)\n",
    "plt.plot(iters, test_losses, label='Test', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "ops = list(op_stats.keys())\n",
    "accs = [(op_stats[op]['correct']/op_stats[op]['total'])*100 for op in ops]\n",
    "plt.bar(range(len(ops)), accs)\n",
    "plt.xticks(range(len(ops)), ops, rotation=45, ha='right')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy by Operation')\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sizes = [sum(1 for r in eval_results if r[3]), sum(1 for r in eval_results if not r[3])]\n",
    "plt.pie(sizes, labels=['Correct', 'Incorrect'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Overall Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part2_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved as 'part2_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights (as required by assignment)\n",
    "torch.save(model.state_dict(), \"model_weights_part2.pth\")\n",
    "\n",
    "print(\"Model saved as: model_weights_part2.pth\")\n",
    "print(\"\\nTo load:\")\n",
    "print(\"  model = GPTLanguageModel()\")\n",
    "print(\"  model.load_state_dict(torch.load('model_weights_part2.pth'))\")\n",
    "print(\"  model.eval()\")\n",
    "\n",
    "# Save sample outputs for report appendix\n",
    "with open('boolean_results_sample.txt', 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"BOOLEAN GPT - SAMPLE PROMPT-OUTPUT PAIRS\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Correct Predictions (Strengths):\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for inp, exp, pred, _ in correct_examples:\n",
    "        f.write(f\"Prompt: {inp}\\n\")\n",
    "        f.write(f\"Output: {pred} ✓\\n\\n\")\n",
    "    \n",
    "    f.write(\"\\nIncorrect Predictions (Weaknesses):\\n\")\n",
    "    f.write(\"-\" * 70 + \"\\n\")\n",
    "    for inp, exp, pred, _ in incorrect_examples:\n",
    "        f.write(f\"Prompt: {inp}\\n\")\n",
    "        f.write(f\"Output: {pred} ✗\\n\")\n",
    "        f.write(f\"Expected: {exp}\\n\\n\")\n",
    "\n",
    "print(\"\\nSample outputs saved to: boolean_results_sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis and Discussion\n",
    "\n",
    "### What Operations Are Learned Correctly?\n",
    "\n",
    "Based on evaluation:\n",
    "- **Excellent performance**: Basic operations (AND, OR, NOT)\n",
    "- **Good performance**: XOR (slightly harder)\n",
    "- **Challenges**: Complex nested parentheses\n",
    "\n",
    "### Comparison with Math GPT:\n",
    "\n",
    "Boolean GPT typically achieves **higher accuracy** because:\n",
    "1. **Smaller state space**: Only 2 values vs infinite numbers\n",
    "2. **Simpler operations**: Truth tables vs arithmetic\n",
    "3. **More training examples per unique pattern**: Higher repetition ratio\n",
    "\n",
    "### Architectural Differences:\n",
    "\n",
    "- **Smaller embeddings**: Boolean needs less representational capacity\n",
    "- **Fewer heads**: Simpler attention patterns\n",
    "- **Longer block size**: Boolean expressions are wordier\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "1. Custom loss function for binary classification\n",
    "2. Tree-based tokenization for parentheses\n",
    "3. Pre-training on truth tables\n",
    "4. Temperature tuning for deterministic output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
